---
title: 'Google data report #1'
author: "Ido Klein"
date: "16/11/2021"
output:
  html_document: 
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE,
	paged.print=TRUE
)
library(sf)
library(fs)
library(tidyverse)
library(RODBC)
library(DBI)
library(assertr)
library(skimr)
library(lubridate)
options(scipen = 10)
Sys.setlocale(locale = "hebrew")
```

## google data report
```{r}
paths <- fs::dir_info("D:/OneDrive_2021_11_10/Google_data_2015_2020/Original_data/access_files2/") %>% 
  pull(path) %>% 
  as.character() 
dbnames <- paths %>% str_split("/") %>% map_chr(~.x[[length(.x)]]) %>% str_sub(end = -7) 
df_db = data.frame(dbname = dbnames, path = paths) %>% 
  filter(!str_detect(dbname,"\\."))
cons <- map(df_db$path,~odbcDriverConnect(paste0("Driver={Microsoft Access Driver (*.mdb, *.accdb)};DBQ=",.x)))

# creation of googletimes files - ran only once, computer has only 16 giga ram

# googletimes <- map(cons,~sqlQuery(.x, "select * from googletimes"))
# googletimes_table1 <- map2(googletimes,dbnames,~.x %>% mutate(dbname = .y)) 
# googletimes_table <- googletimes_table1[googletimes_table1 %>% map(~nrow(.x)!=0) %>% as.logical()] %>% 
# bind_rows()
# nr <- nrow(googletimes_table)
# map(0:61,~googletimes_table[(1 + .x*1000000):min(1000000 + .x*1000000,nr),] %>% 
# write_csv(paste0("google_times",.x,".csv")))

# unifying of googletimes files - ran only once
# googletimes_table <- googletimes %>% bind_rows()
# lst <- dir_info() %>% 
#   filter(str_detect(path,".csv")) %>% 
#   pull(path) %>% 
#   as.character() %>% 
#   map(read_csv)
# googletimes_table <- lst %>% 
#   bind_rows() 
# rm(lst)
# gc()
# write_csv(googletimes_table,"googletimes.csv")
# almost 5 giga, much less than before (around 12)
# saved google data
googletimes<- read_csv("D:/OneDrive_2021_11_10/google_data_anal/googletimes.csv")
# add relevant columns
googletimes$date <- as_date(googletimes$timestamp)
googletimes$from_epoch <- as.numeric(googletimes$timestamp)
# change relevant columns
googletimes$project <-  as.factor(googletimes$project)
googletimes$dbname <-  as.factor(googletimes$dbname)
# gc()

```
I have recieved 30 access database files in the size of ~14 gb.  
one of the files (backup of uptodateprojects,~2gb) was corrupted and could not be processed.  Jordan Katz will resend it. 
the first assignment was converting the actual google data (stord in each db under the "googletimes" table) to one file. the unification of all files created a ~61 million rows file.  
data structure:
```{r echo=TRUE,include = TRUE}
head(googletimes)
```
number of rows:
```{r echo=TRUE,include = TRUE}
nrow(googletimes)
```
When trying to find a unique identifier for each row, I found that the ID column is useless
```{r echo=TRUE,include = TRUE}
googletimes %>% 
  filter(project == "beinironi",ID == 1)
```
and that some of the projects are spread around multiple dbs
```{r echo=TRUE,include = TRUE}
googletimes %>% 
  filter(project == "beinironi") %>% 
  count(dbname) %>% 
  arrange(-n)
```
therefore I made each row distinct by using all of its values, besides the dbname and id
```{r echo=TRUE, include = TRUE}
distincts <- googletimes %>% 
  select(-ID,-dbname) %>%
  distinct()
```
I now evalute the number of projects and the number of rows in each of them. there are 232 projects, the largest 10 take ~75% of all rows. ~6% have no identifier. 
```{r echo=TRUE,include = TRUE,paged.print = TRUE}
largest_proj2 <- distincts %>% 
  count(project) %>% 
  arrange(-n)
largest_proj2
```
time coverage of 10 largest projects is missing april 2018 to may 2019. this data exists, I have saved some of it about two years ago - mainly the beinironi, however it does not appear in matat files
```{r echo=TRUE,include = TRUE}
distincts %>% 
  filter(project %in% largest_proj2$project[1:10]) %>% 
  count(project,date) %>% 
  arrange(-n) %>% 
  ggplot(aes(x=date,y=n)) + 
  geom_col() + 
  facet_grid(project~.) + 
  theme(strip.text.y = element_text(angle = 0),
        axis.text.x = element_text(angle = 90)) + 
  scale_x_date(date_breaks = "1 month")
```
  
regarding other tables - very big mess. some of the dbs have certain tables, other dont. the kind we are interested in is the meta data for the project and maslulid
```{r echo=TRUE,include = TRUE,paged.print = TRUE}
table_status <- map(cons,~sqlTables(.x, tableType = "TABLE")) %>% 
  bind_rows() %>% 
  left_join(df_db, by = c("TABLE_CAT" = "path")) %>% 
  select(dbname,TABLE_NAME) %>% 
  mutate(num = 1) %>% 
  spread(TABLE_NAME,num)
table_status
```
when calculating this, I have only 110 unique projects, as opposed to 232 existing in the google times table. that means that for more than a hundred project, a simple join of each maslul can not be carried out. the number of maslulim per project is presented in this table.
```{r}
# get all calemders, see whether there are problems
calendar <- map(cons,~sqlQuery(.x, "select * from calendar"))
has_calendar <- map(calendar,~class(.x) =="data.frame") %>% unlist()
calendar_table1 <- map2(calendar[has_calendar],df_db$dbname[has_calendar],function(x,y){x %>% mutate(dbname = y)})
calendar_table <- calendar_table1[calendar_table1 %>% map(~nrow(.x)!=0) %>% as.logical()] %>%
  bind_rows()
gc()
# got a distinct calender. only for 2016-2017
dis_calender <- calendar_table %>% 
  select(-dbname) %>% 
  distinct()
# get all newroutes see whether there are problems
newroutes <- map(cons,~sqlQuery(.x, "select * from newroutes"))
has_newroutes <- map(newroutes,~class(.x) =="data.frame") %>% unlist()
newroutes_table1 <- map2(newroutes[has_newroutes],df_db$dbname[has_newroutes],function(x,y){x %>% mutate(dbname = y)})
# map(newroutes_table1,~.x$מסלול %>% class())
newroutes_table1[[26]]$מסלול <- as.character(newroutes_table1[[26]]$מסלול)
newroutes_table <- newroutes_table1[newroutes_table1 %>% map(~nrow(.x)!=0) %>% as.logical()] %>%
  bind_rows()
# got distinct newroutes, however they are lacking, most likely to be found in "maslulim"
dis_newroutes <- newroutes_table %>% 
  select(-dbname) %>% 
  distinct()

# testrips - like googletimes, however meant to test the db
testrips  <- map(cons,~sqlQuery(.x, "select * from testrips"))
has_testrips<- map(testrips,~class(.x) =="data.frame") %>% unlist()
testrips_table1 <- map2(testrips[has_testrips],df_db$dbname[has_testrips],function(x,y){x %>% mutate(dbname = y)})

testrips_table <- testrips_table1[testrips_table1 %>% map(~nrow(.x)!=0) %>% as.logical()] %>%
  bind_rows()
# got distinct testrips, don't know what to do with them
dis_testrips  <- testrips_table %>% 
  select(-dbname) %>% 
  distinct()

# trips - like newroutes
trips  <- map(cons,~sqlQuery(.x, "select * from trips"))
has_trips <- map(trips,~class(.x) =="data.frame") %>% unlist()
trips_table1 <- map2(trips[has_trips],df_db$dbname[has_trips],function(x,y){x %>% mutate(dbname = y)})
# map(trips_table1,~.x$מסלול %>% class())
trips_table1[[22]]$מסלול <- as.character(trips_table1[[22]]$מסלול)
trips_table <- trips_table1[trips_table1 %>% map(~nrow(.x)!=0) %>% as.logical()] %>%
  bind_rows()
# got distinct trips, they are like newroutes. however, querying a matrix is differnt than a simple api call, need to split
dis_trips <- trips_table %>% 
  select(-dbname) %>% 
  distinct()
not_all_na <- function(x) any(!is.na(x))
matrices <- dis_trips %>% 
  filter(!is.na(project)) %>% 
  select_if(not_all_na) 
dis_trips_not_matrices <- dis_trips %>% 
  filter(is.na(project)) %>% 
  select_if(not_all_na) 


# trips_all - like newroutes
trips_all <- map(cons,~sqlQuery(.x, "select * from trips_all"))
has_trips_all <- map(trips_all,~class(.x) =="data.frame") %>% unlist()
trips_all_table1 <- map2(trips_all[has_trips_all],df_db$dbname[has_trips_all],function(x,y){x %>% mutate(dbname = y)})

# map(trips_all_table1,~.x$מסלול %>% class())
trips_all_table1[[26]]$מסלול <- as.character(trips_all_table1[[26]]$מסלול)
trips_all_table <- trips_all_table1[trips_all_table1 %>% map(~nrow(.x)!=0) %>% as.logical()] %>%
  bind_rows()
# got distinct trips, they are like newroutes. however, querying a matrix is differnt than a simple api call, need to split
dis_trips_all <- trips_all_table %>% 
  select(-dbname) %>% 
  distinct()


# explore trips1, wait and waiting times
trips1 <- map(cons,~sqlQuery(.x, "select * from trips1"))
has_trips1 <- map(trips1,~class(.x) =="data.frame") %>% unlist()
trips1_table1 <- map2(trips1[has_trips1],df_db$dbname[has_trips1],function(x,y){x %>% mutate(dbname = y)})
trips1_table <- trips1_table1[trips1_table1 %>% map(~nrow(.x)!=0) %>% as.logical()] %>%
  bind_rows()
dis_trips1 <- trips1_table %>% 
  select(-dbname) %>% 
  distinct()
# have only 110 projects, out of 232 with data. concerning

meta_data_proj <- bind_rows(dis_trips_not_matrices,dis_newroutes,dis_trips_all,dis_trips1) %>% 
  select_if(not_all_na) %>% 
  select(`פרוייקט`,maslulid) %>% 
  distinct() %>% 
  count(`פרוייקט`) %>% 
  arrange(-n)
meta_data_proj
```
projects that can be joined:
```{r echo=TRUE,include = TRUE,paged.print = TRUE}
largest_proj2 %>% 
  filter(project %in% meta_data_proj$פרוייקט)
```

projects that can't be joined:
```{r echo=TRUE,include = TRUE,paged.print = TRUE}
largest_proj2 %>% 
  filter(!project %in% meta_data_proj$פרוייקט)
```

next assignments:  
1. find missing metadata regarding maslulim  
2. talk to matat regarding missing year  
3. create geometric features for each maslul  
4. store all relevant data in a gpkg (geographic local database serving as one package)  
5. document the tables and rows  
6. perform data quality assurance (if I remember correctly, there might be a 2 hour lag)  
7. prepare for distribution  
8. talk to lishcah mishpatit regarding distribution  